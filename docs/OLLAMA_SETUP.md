# Настройка Ollama для DevDataSorter

Этот проект теперь использует Ollama вместо OpenAI API для классификации контента. Ollama позволяет запускать языковые модели локально без необходимости в API ключах.

## Установка Ollama

### Windows
1. Скачайте Ollama с официального сайта: https://ollama.ai/
2. Запустите установщик и следуйте инструкциям
3. После установки Ollama будет доступен через командную строку

### macOS
```bash
brew install ollama
```

### Linux
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

## Настройка модели

1. Запустите Ollama сервер:
```bash
ollama serve
```

2. Установите рекомендуемую модель (в новом терминале):
```bash
ollama pull llama2
```

Или для более быстрой модели:
```bash
ollama pull llama2:7b-chat
```

## Конфигурация проекта

1. Создайте файл `.env` на основе `.env.example`:
```bash
cp .env.example .env
```

2. Настройте параметры Ollama в `.env`:
```
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# Telegram Bot Token (обязательно)
TELEGRAM_BOT_TOKEN=your_telegram_bot_token_here
```

## Проверка работы

1. Убедитесь, что Ollama запущен:
```bash
ollama list
```

2. Протестируйте API:
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Hello, world!",
  "stream": false
}'
```

3. Запустите бота:
```bash
python run.py
```

## Альтернативные модели

Вы можете использовать другие модели, доступные в Ollama:

- `llama2:7b-chat` - быстрая модель для чата
- `llama2:13b-chat` - более точная, но медленная
- `codellama` - специализированная для кода
- `mistral` - компактная и эффективная

Для смены модели:
1. Установите новую модель: `ollama pull model_name`
2. Обновите `OLLAMA_MODEL` в файле `.env`
3. Перезапустите бота

## Устранение проблем

### Ollama не запускается
- Проверьте, что порт 11434 свободен
- Убедитесь, что у вас достаточно RAM (минимум 8GB для llama2:7b)

### Модель работает медленно
- Используйте меньшую модель (llama2:7b вместо llama2:13b)
- Убедитесь, что у вас достаточно RAM
- Рассмотрите использование GPU (если поддерживается)

### Ошибки подключения
- Проверьте, что Ollama сервер запущен: `ollama serve`
- Убедитесь, что URL в `.env` правильный: `http://localhost:11434`
- Проверьте, что модель установлена: `ollama list`

## Преимущества Ollama

- ✅ Бесплатно и без лимитов
- ✅ Работает локально (приватность данных)
- ✅ Не требует интернет-соединения
- ✅ Множество доступных моделей
- ✅ Простая установка и настройка

## Fallback режим

Если Ollama недоступен, бот автоматически переключится на базовую классификацию по паттернам. Это обеспечивает работоспособность даже без AI модели.